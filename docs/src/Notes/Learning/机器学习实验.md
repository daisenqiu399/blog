---
updateTime: "2024-12-10 21:09"
desc: "机器学习实验鸢尾花聚类"
tags: "机器学习"
outline: deep
---
:::info
**🐶对“鸢尾花”（Iris）数据集的K-means聚类分析，并利用PCA（主成分分析）降维来可视化聚类结果。以下是各部分代码的逐步解析**
:::
**🎓Contributors:`蔡浪英`，`张顺`，`叶真凤`，`宋选为`，`谢梦月`**

### **1、导入库**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import warnings
warnings.filterwarnings(action = 'ignore')
%matplotlib inline
plt.rcParams['font.sans-serif']=['SimHei']  #解决中文显示乱码问题
plt.rcParams['axes.unicode_minus']=False

```

导入所需的Python库，包括数据处理（`numpy`, `pandas`），绘图（`matplotlib`），机器学习（`sklearn`）等。

`warnings.filterwarnings(action='ignore')` 用于忽略警告信息。

`%matplotlib inline` 是Jupyter Notebook中用于显示图表的命令。

### **2、导入数据集**

```python
data = pd.read_csv('iris.csv')
X1 = data[['sepal_length','sepal_width','petal_length','petal_width']]
N = X1['sepal_length'].size

```

**加载数据**：`iris.csv` 是一个经典的鸢尾花数据集，包含了4个特征：

- 花萼长度 (`sepal_length`)
- 花萼宽度 (`sepal_width`)
- 花瓣长度 (`petal_length`)
- 花瓣宽度 (`petal_width`)
  它的目标是使用这些特征将数据点分为不同的簇。

**提取特征数据**：只选择了这四列特征进行聚类分析，忽略其他可能存在的列（如类别标签）。

**记录样本数量**：`N` 记录了样本的总数。方便后续的循环操作。

------

### **3. 选择最佳聚类数：轮廓系数分析**

```python
K = range(2, 11)
silhouettescore = []
for k in K:
    KM = KMeans(n_clusters=k, random_state=0)
    KM.fit(X1)
    silhouettescore.append(silhouette_score(X1, KM.labels_))
plt.plot(K, silhouettescore)
plt.xlabel("聚类数目K")
plt.ylabel("轮廓系数")
plt.title("轮廓系数与聚类数目")

```



1. **轮廓系数简介**：
   - **定义**：轮廓系数衡量样本点与其所属簇的紧密程度以及与最近簇的分离程度。
   - **范围**：[-1, 1]。值越接近1，聚类效果越好；值接近0表示簇之间边界模糊；值为负数表示样本被错误分配。
2. **计算轮廓系数**：
   - 对每个K值进行K-means聚类。
   - 使用 `silhouette_score` 计算整个数据集的平均轮廓系数。
3. **绘制图表**：
   - 横轴：聚类数目K。
   - 纵轴：对应的轮廓系数。
   - 图中轮廓系数的峰值通常对应于最佳的K值。

#### 输出：

通过观察图表，找到轮廓系数最高的K值作为推荐的聚类数。
<img src="/machinelearing1.png" alt="VitePress Logo" width="600" />


------

**3. 选择最佳聚类数：肘部法则（SSE分析）**

```python
k_range = range(2, N)
sse = []
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=0)
    kmeans.fit(X1)
    sse.append(kmeans.inertia_)

plt.figure(figsize=(13, 4))
plt.plot(k_range, sse, 'bo-')
plt.xlabel('聚类数目K')
plt.ylabel('SSE')
plt.title('碎石图')
plt.show()

```

1. **SSE简介**：
   - **定义**：SSE (Sum of Squared Errors) 是每个样本到其最近簇中心的距离平方和。
   - **范围**：值越小，簇内样本越紧凑。
   - K值增加时，SSE必然减小（但减少的幅度会逐渐趋缓）。
2. **计算SSE**：
   - 对每个K值进行K-means聚类。
   - 使用 `kmeans.inertia_` 获取当前聚类的SSE。
3. **绘制图表**：
   - 横轴：聚类数目K。
   - 纵轴：对应的SSE。
   - 图中`SSE`急剧下降的拐点”即为最佳的K值。

#### 输出：

通过观察碎石图，找到“SSE急剧下降拐点”的K值作为推荐的聚类数。

<img src="/machinelearing2.png" alt="VitePress Logo" width="600" />

4. **聚类分析与可视化**

```python
bk = 2
def MyDraw(title, labels, i):
    pca = decomposition.PCA(n_components=2)
    pca.fit(X1)
    y = pca.transform(X1)
    markers = ['o', '*', '+', '>']
    ax = plt.subplot(1, 2, i)
    for k, m in zip(range(bk), markers):
        ax.scatter(y[labels == k, 0], y[labels == k, 1], marker=m, s=80, label='小类:' + str(k))
    ax.set_title(title, fontsize=14)
    ax.set_xlabel('F1', fontsize=14)
    ax.set_ylabel('F2', fontsize=14)
    ax.legend()

fig = plt.figure(figsize=(15, 6))
BKM = KMeans(n_clusters=bk, random_state=0)
BKM.fit(X1)
MyDraw('K-均值聚类解', BKM.labels_, 2)

```



1. **K-means聚类**：
   - 使用之前选择的K值进行K-means聚类（此处 `bk=2`）。
   - 通过 `BKM.fit(X1)` 进行模型训练，`BKM.labels_` 获得每个样本的簇标签。
2. **PCA降维**：
   - 使用PCA将四维数据降至二维，便于在平面上可视化聚类结果。
   - `y` 是降维后的二维数据。
3. **绘制散点图**：
   - 按簇标签对数据点进行颜色与形状区分。
   - 添加标题和坐标轴标签，帮助解释图表。

<img src="/machinelearing3.png" alt="VitePress Logo" width="600" />

------

**5. 输出结果与保存**

```python
centroids = BKM.cluster_centers_
print("The cluster centers are:")
for centroid in centroids:
    print(centroid)
labels = BKM.labels_
data['Cluster'] = labels
data.to_excel('clustering_results.xlsx', index=False)

```

**输出聚类中心**：

- 每个簇中心是多维空间中的一个点，表示簇的“重心”。
- 使用 `BKM.cluster_centers_` 获取这些中心点。

**保存聚类结果**：

- 将聚类标签 `labels` 添加到原始数据集中。
- 使用 `to_excel` 将结果保存为Excel文件，以便后续分析。

### **总结与分析**

1. **K-means算法流程**：
   - **输入**：样本数据和预设的聚类数K。
   - **输出**：每个样本的簇标签及簇中心。
2. **K值的选择**：
   - 使用**轮廓系数**和**肘部法则**两种方法综合判断最佳的K值。
3. **降维与可视化**：
   - 使用PCA降维后，在二维空间中直观地展示聚类效果。
4. **结果保存**：
   - 将每个样本的簇标签导出到Excel文件中，便于后续的分析与应用。

## 代码修改

### 1. **改进数据预处理与标准化**

聚类算法对于数据的尺度非常敏感。不同特征之间的尺度差异可能会导致聚类结果的偏差。因此，**数据标准化**是非常重要的一步。在原始代码中没有进行标准化，可能会影响聚类效果。

**改进措施：**

- 使用 `StandardScaler` 对数据进行标准化，以保证每个特征的均值为 0，标准差为 1。
- 这样可以避免尺度较大的特征主导聚类结果。

**代码修改：**

```python
from sklearn.preprocessing import StandardScaler
# 标准化数据
scaler = StandardScaler()
X1_scaled = scaler.fit_transform(X1)

```

**理由：**

- 聚类算法（尤其是 KMeans 和 EM 聚类）假设所有特征的尺度是相同的，否则距离计算可能会受到某些特征的影响较大，导致不准确的聚类。

### 2. **使用 `silhouette_score` 进行聚类评估**

`silhouette_score` 是衡量聚类质量的一个有效指标，可以在选择最优聚类数目时提供帮助。代码中已使用它来选择最佳的 `K` 值，但没有导入相关模块。

**改进措施：**

- 导入 `silhouette_score` 模块，计算并显示每个 `K` 值对应的 `silhouette_score`。
- 在选择最佳 `K` 值时，不仅考虑 `SSE`（误差平方和），还可以结合 `silhouette_score`，从而综合考虑聚类的内部一致性。

**代码修改：**

```python
from sklearn.metrics import silhouette_score
silhouettescore = []
for k in K:
    KM = KMeans(n_clusters=k, random_state=0)
    KM.fit(X1_scaled)
    silhouettescore.append(silhouette_score(X1_scaled, KM.labels_))

```



**理由：**

- 仅依赖 SSE（误差平方和）来选择最佳 `K` 值容易导致过拟合，`silhouette_score` 可以帮助评估每个样本是否被正确地聚类，并反映了聚类的紧密度和分离度。

### 3. **使用 `PCA`（主成分分析）进行降维**

对于高维数据（例如本例中的四个特征：`sepal_length`，`sepal_width`，`petal_length`，`petal_width`），使用降维方法如 **PCA** 可以有效地可视化聚类结果，同时减少计算复杂度。

**改进措施：**

- 在聚类前使用 **PCA** 降到 2 个主成分，然后进行可视化。
- 显示不同聚类的结果，确保不同类别的数据点有明显的分离。

**代码修改：**

```python
pca = decomposition.PCA(n_components=2)
X1_pca = pca.fit_transform(X1_scaled)

```



**理由：**

- 降维后可以更直观地观察聚类的效果，尤其是在二维空间中。`PCA` 可以提取数据中最重要的特征，并且能减少计算复杂度。

### 4. **优化聚类的评估与可视化**

聚类的可视化非常重要，特别是在选择不同 `K` 值时，我们希望看到聚类结果的可解释性和一致性。

**改进措施：**

- 添加更多的可视化方法，除了展示聚类中心和类别分布，还可以绘制 **聚类结果的轮廓图**。
- 可以尝试使用 `AgglomerativeClustering` 或 `GaussianMixture` 等其他聚类算法，并比较它们的表现。

**代码修改：**

```python
from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=bk, random_state=0)
gmm_labels = gmm.fit_predict(X1_scaled)

```



### 5. **增加结果输出与保存**

目前的代码中，将聚类结果存储为 Excel 文件，但没有保存其他相关信息如聚类的性能评分、不同 `K` 值的评估指标等。

**改进措施：**

- 保存更多的聚类评估指标，如 `silhouette_score`，以及每种聚类算法的结果。
- 为每个聚类数目输出其对应的评估结果，便于后续分析。

**代码修改：**

```python
clustering_results = pd.DataFrame({
    'K': K,
    'Silhouette Score': silhouettescore
})
clustering_results.to_excel('clustering_evaluation.xlsx', index=False)

```

## 结果如图
<img src="/machinelearing4.png" alt="VitePress Logo" width="600" />
